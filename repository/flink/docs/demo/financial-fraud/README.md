# Flink Financial Fraud Demo

This demo follows the outline provided by [DC/OS's](https://github.com/dcos/demos/tree/master/flink-k8s/1.11) demo

## Architecture

![Financial transaction processing demo architecture](https://github.com/dcos/demos/raw/master/flink-k8s/1.11/img/kafka-flink-arch.png)

This demo implements a data processing infrastructure with KUDO that is able to spot money laundering. In the context of money laundering, we  want to detect amounts larger than $10.000 transferred between two accounts, even if that amount is split into many small batches.  See also [US](https://www.fincen.gov/history-anti-money-laundering-laws) and [EU](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32015L0849) legislation and regulations on this topic for more information.

The architecture follows more or less the [SMACK stack architecture](https://mesosphere.com/blog/smack-stack-new-lamp-stack/):
- Events: Event are being generated by a small [Golang generator](https://github.com/dcos/demos/blob/master/flink/1.11/generator/generator.go). The events are in the form 'Sunday, 23-Jul-17 01:06:47 UTC;66;26;7810', where the first field '23-Jul-17 01:06:47 UTC' represents the (increasing) timestamp of transactions; the second field '66' represent the sender account; the third field the receiver account; and the fourth field represent the dollar amount transferred during that transaction.
- Ingestion: The generated events are being ingested and buffered by a Kafka queue with the default topic 'transactions'. Being a Microservice we will deploy the data-generator on kubernetes.
- Stream Processing: As we require fast response times, we use Apache Flink as a Stream processor running the [FinancialTransactionJob](https://github.com/dcos/demos/tree/master/flink/1.10/flink-job/src/main/java/io/dcos).
- Storage: Here we diverge a bit from the typical SMACK stack setup and don't write the results into a data store such as Apache Cassandra. Instead, we write the results again into a Kafka Stream (default: 'fraud'). Note, that Kafka also offers data persistence for all unprocessed events.
- Actor: In order to view the results we use again a small [Golang viewer](https://github.com/dcos/demos/blob/master/flink/1.11/actor/actor_viewer.go) which simply reads and displays the results from the output Kafka stream. Being a Microservice we will deploy the viewer on kubernetes.

## Prerequisites

Before you get started, make sure you have a cluster at hand with enough resources.

### Run on [KinD](https://github.com/kubernetes-sigs/kind)

```bash
# Use make from operators repo:
$ make create-cluster

# or modify your own KinD cluster:
$ kind create cluster
$ export KUBECONFIG="$(kind get kubeconfig-path --name="kind")"
$ kubectl delete storageclass standard
$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
$ kubectl annotate storageclass --overwrite local-path storageclass.kubernetes.io/is-default-class=true
```
*note:* cpus: 4 and mem: 8Gb was sufficient resources on docker for KinD to work (which is not the default)

### Run on [minikube](https://github.com/kubernetes/minikube)

```bash
$ minikube start --vm-driver=hyperkit --cpus=6 --memory=9216 --disk-size=10g
```

### Install KUDO and the KUDO CLI

- Have current KUDO CLI `v0.15.x` [installed](https://kudo.dev/docs/cli.html#setup-the-kudo-kubectl-plugin)
    - Run `kubectl kudo version`, the output should look something like:
        ```bash
        $ kubectl kudo version
        KUDO Version: version.Info{GitVersion:"0.15.0", GitCommit:"3a6cdfa6", BuildDate:"2020-06-26T12:20:36Z", GoVersion:"go1.13", Compiler:"gc", Platform:"darwin/amd64"}
        ```
    - If not, upgrade to the latest version via `brew upgrade kudo-cli` on macOS or follow the [installation instructions](https://kudo.dev/docs/cli.html#setup-the-kudo-kubectl-plugin) on other platforms.
    - Have current KUDO `v0.15.x` installed on your cluster:
    - If you have KUDO already installed to the `kudo-system` namespace check its current Docker image tag to verify the version you are running:
        - Make sure the output of `kubectl get pod kudo-controller-manager-0 -n kudo-system -o jsonpath='{.spec.containers[0].image}'` shows at least `v0.15.x` as Docker tag:
        ```bash
        $ kubectl get pod kudo-controller-manager-0 -n kudo-system -o jsonpath='{.spec.containers[0].image}'
        kudobuilder/controller:v0.15.0
        ```
    - If not, use the following commands to install KUDO `v0.15.x` to your cluster:
        ```bash
        $ kubectl kudo init --unsafe-self-signed-webhook-ca
        ```
      the `--unsafe-self-signed-webhook-ca` allows KUDO to run without the [cert-manager](https://github.com/jetstack/cert-manager) but rather with a self-signed CA bundle which is fine for the purposes of this demo.

## Start the Demo

Install the Flink `financial-fraud` demo from the main repository directory.

Get the KUDO Operators repository and cd into the cloned repository:

```bash
$ git clone git@github.com:kudobuilder/operators.git
$ cd operators
```

Install the Flink demo objects straight out of the repository:

```bash
$ kubectl kudo install repository/flink/docs/demo/financial-fraud/demo-operator --instance flink-demo
operator default/flink-demo created
operator default/kafka created
operatorversion default/kafka-1.2.0 created
operator default/zookeeper created
operatorversion default/zookeeper-0.3.0 created
operator default/flink created
operatorversion default/flink-0.2.1 created
operatorversion default/flink-demo-0.1.5 created
instance default/flink-demo created
```

The demo relies on [Zookeeper](https://github.com/kudobuilder/operators/tree/master/repository/zookeeper), [Kafka](https://github.com/kudobuilder/operators/tree/master/repository/kafka) and [Flink](https://github.com/kudobuilder/operators/tree/master/repository/flink) operators which are all automatically installed as part of the flink-demo operator.

To see the status of the deploy plan for the Zookeeper operator we can utilize the CLI via:

```bash
$ kubectl kudo plan status --instance zk
Plan(s) for "zk" in namespace "default":
.
└── zk (Operator-Version: "zookeeper-0.3.0" Active-Plan: "deploy")
    ├── Plan deploy (serial strategy) [IN_PROGRESS]
    │   ├── Phase zookeeper [IN_PROGRESS]
    │   │   └── Step deploy (IN_PROGRESS)
    │   └── Phase validation [PENDING]
    │       ├── Step validation (PENDING)
    │       └── Step cleanup (PENDING)
    └── Plan validation (serial strategy) [NOT ACTIVE]
        └── Phase connection (serial strategy) [NOT ACTIVE]
            └── Step connection (serial strategy) [NOT ACTIVE]
                ├── connection [NOT ACTIVE]
                └── cleanup [NOT ACTIVE]
```

If the Zookeeper Operator was successfully installed its plan status will show `COMPLETE`:

```bash
$ kubectl kudo plan status --instance zk
Plan(s) for "zk" in namespace "default":
.
└── zk (Operator-Version: "zookeeper-0.3.0" Active-Plan: "deploy")
    ├── Plan deploy (serial strategy) [COMPLETE]
    │   ├── Phase zookeeper [COMPLETE]
    │   │   └── Step deploy (COMPLETE)
    │   └── Phase validation [COMPLETE]
    │       ├── Step validation (COMPLETE)
    │       └── Step cleanup (COMPLETE)
    └── Plan validation (serial strategy) [NOT ACTIVE]
        └── Phase connection (serial strategy) [NOT ACTIVE]
            └── Step connection (serial strategy) [NOT ACTIVE]
                ├── connection [NOT ACTIVE]
                └── cleanup [NOT ACTIVE]
```

Next, the Kafka operator will start its `deploy` plan, when completed we will see its status change to `COMPLETE` as well:

```bash
$ kubectl kudo plan status --instance kafka
Plan(s) for "kafka" in namespace "default":
.
└── kafka (Operator-Version: "kafka-1.2.0" Active-Plan: "deploy")
    ├── Plan deploy (serial strategy) [COMPLETE]
    │   └── Phase deploy-kafka [COMPLETE]
    │       └── Step deploy (COMPLETE)
    └── Plan not-allowed (serial strategy) [NOT ACTIVE]
        └── Phase not-allowed (serial strategy) [NOT ACTIVE]
            └── Step not-allowed (serial strategy) [NOT ACTIVE]
                └── not-allowed [NOT ACTIVE]
```

Lastly, the Flink operator needs to be installed. We wait for its status to be completed similar to Zookeeper and Kafka:

```bash
$ kubectl kudo plan status --instance flink
Plan(s) for "flink" in namespace "default":
.
└── flink-demo-flink (Operator-Version: "flink-0.2.1" Active-Plan: "deploy")
    └── Plan deploy (serial strategy) [COMPLETE]
        └── Phase flink (serial strategy) [COMPLETE]
            └── Step jobmanager (COMPLETE)
```

Now, if we look at the overall Flink-Demo Operator status we see the combined status of all plans and phases run for the
Flink demo:

```bash
$ kubectl kudo plan status --instance flink-demo
Plan(s) for "flink-demo" in namespace "default":
.
└── flink-demo (Operator-Version: "flink-demo-0.1.4" Active-Plan: "deploy")
    └── Plan deploy (serial strategy) [COMPLETE]
        ├── Phase dependencies [COMPLETE]
        │   ├── Step zookeeper (COMPLETE)
        │   └── Step kafka (COMPLETE)
        ├── Phase flink-cluster [COMPLETE]
        │   └── Step flink (COMPLETE)
        ├── Phase demo [COMPLETE]
        │   ├── Step gen (COMPLETE)
        │   └── Step act (COMPLETE)
        └── Phase flink-job [COMPLETE]
            └── Step submit (COMPLETE)
```

Wonderful, all phases completed and our Flink job even got submitted.

## View the Demo Application

In order to verify that the demo is running, let's have a look at the Flink dashboard:

- Run `kubectl proxy` to make the dashboard available
- Access in your web-browser: http://127.0.0.1:8001/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy/#/overview

To see if the job was submitted successfully:

```bash
$ kubectl logs $(kubectl get pod -l job-name=submit-flink-job -o jsonpath="{.items[0].metadata.name}")
DOWNLOAD_URL: https://downloads.mesosphere.com/dcos-demo/flink/flink-job-1.0.jar FILE: flink-job-1.0.jar JOBMANAGER: flink-demo-flink-jobmanager
fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz
(1/7) Installing ca-certificates (20190108-r0)
(2/7) Installing nghttp2-libs (1.35.1-r0)
(3/7) Installing libssh2 (1.8.2-r0)
(4/7) Installing libcurl (7.64.0-r2)
(5/7) Installing curl (7.64.0-r2)
(6/7) Installing oniguruma (6.9.1-r0)
(7/7) Installing jq (1.6-r0)
Executing busybox-1.29.3-r10.trigger
Executing ca-certificates-20190108-r0.trigger
OK: 16 MiB in 25 packages
{"filename":"/ha/data/flink-web-upload/1044f898-66b0-4c71-a59c-602cdd53a7b9_flink-job-1.0.jar","status":"success"}Wed Jul 10 18:54:50 UTC 2019
Found jar 1044f898-66b0-4c71-a59c-602cdd53a7b9_flink-job-1.0.jar
RESPONSE: null
SUBMITTED JOB!
```

To get the fraud output from the actor:

```bash
$ kubectl logs $(kubectl get pod -l actor=flink-demo -o jsonpath="{.items[0].metadata.name}")
```

The output will look like:

```
Broker:   kafka-kafka-0.kafka-svc:9093
Topic:   fraud

Detected Fraud:   TransactionAggregate {startTimestamp=0, endTimestamp=1562784331000, totalAmount=11612:
Transaction{timestamp=1562784330000, origin=1, target='5', amount=5175}
Transaction{timestamp=1562784331000, origin=1, target='5', amount=6437}}

Detected Fraud:   TransactionAggregate {startTimestamp=0, endTimestamp=1562784349000, totalAmount=16917:
Transaction{timestamp=1562784339000, origin=0, target='7', amount=9028}
Transaction{timestamp=1562784349000, origin=0, target='7', amount=7889}}
```

Congratulations, you just installed a highly available Flink cluster that runs a financial fraud detection job!

## Cleanup

To successfully uninstall the demo follow those steps:

- Delete the `flink-demo` instance: `kubectl delete instance flink-demo`
- Delete all PVCs:
    - For Kafka: `kubectl delete pvc -l kudo.dev/instance=kafka`
    - For Zookeeper: `kubectl delete pvc -l kudo.dev/instance=zk`

## Command Reference

The required demo scripts without supportive text.  Please read above for explanations of each of these commands.

```bash
# create kind cluster
kind create cluster
export KUBECONFIG="$(kind get kubeconfig-path --name="kind")"
kubectl delete storageclass standard
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
kubectl annotate storageclass --overwrite local-path storageclass.kubernetes.io/is-default-class=true

# install KUDO manager
kubectl kudo init --unsafe-self-signed-webhook-ca --wait

# verify correct version of manager
kubectl get pod kudo-controller-manager-0 -n kudo-system -o jsonpath='{.spec.containers[0].image}'

# install demo
kubectl kudo install repository/flink/docs/demo/financial-fraud/demo-operator --instance flink-demo

# watch plans
kubectl kudo plan status --instance zk
kubectl kudo plan status --instance kafka
kubectl kudo plan status --instance flink
kubectl kudo plan status --instance flink-demo

# watch pod deployments
kubectl get pods -w

# view the demo application
kubectl proxy
open http://127.0.0.1:8001/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy/#/overview

kubectl logs $(kubectl get pod -l job-name=submit-flink-job -o jsonpath="{.items[0].metadata.name}")

kubectl logs $(kubectl get pod -l actor=flink-demo -o jsonpath="{.items[0].metadata.name}")

```
